---
slug: an-ipfs-adventure
title: An IPFS adventure
description: Lessons learned from deploying this blog on IPFS
authors: madoke
image: /assets/images/iceberg-a777f4bac14629d69895f3d6f644451c.jpg
tags: [software engineering, ipfs, web3, decentralized]
---

import ImageWithAttribution from "../src/theme/ImageWithAttribution";

Content sharing is the most common internet activity, easy and accessible to everyone thanks to cloud providers, social networks, blogging platforms etc. They spoil us with cheap, reliable and high available services, while in return we give them our data. The fuzz about internet centralization and the cryptocurrency boom brought a wave of investment and the promise to build a new decentralized internet ([Web3](https://blog.cloudflare.com/what-is-web3/)) that tackles such privacy issues, and gives the [control back to the users](https://blog.coinbase.com/understanding-web-3-a-user-controlled-internet-a39c21cf83f3). We're still very far from it, but it starts to look less utopian, given the number of people working in the space at the moment, and all the new platforms and technologies currently being developed with the intention of making Web3 a reality. One such tech is the [Inter Planetary File System](https://ipfs.io) (IPFS) that aims to fix the problem of content addressing and streaming on a peer-to-peer network. Drawn by the hype, and curiosity, I've set myself on the journey to build an IPFS cluster for hosting this website. Below I'll discuss the ups and downs and what I've learned about the current state of the decentralized web.

<!--truncate-->

import SpiderWebImage from './img/spider-web.jpg'

<ImageWithAttribution
  src={SpiderWebImage}
  sourceName={'Unsplash'}
  authorName={'Shubham Sharma'}
  imageUrl={'https://unsplash.com/photos/OOm6Spsk3ug'}/>

## TL/DR

- I strongly believe self-hosting is somewhere down the road to decentralization and tried to go that route;
- Soon it became obvious that full-blown self-hosting would create other complex problems and deviate myself from the original goal, so I've accepted the tradeoff of using cloud servers to run the IPFS cluster;
- DNSLink and public IPFS HTTP gateways try to bridge the gap between Web2 and Web3, and the combination is not much different from the traditional web stack, and dramatically slower;
- Cloud providers, CDNs and DNS registrars can still take my website down or at least degrade it (So much for self-hosting and decentralization);
- I ended up falling back to the traditional static website hosting on the cloud, but still offer a solution for users willing to run their own IPFS HTTP gateways via Brave or IPFS Companion;
- We need to change the way modern browsers access online content so that we're able to move away from the traditional client-server architecture to a more peer-to-peer content sharing model;

## Self-hosting

Unlike bitcoin and other blockchain based systems that replicate the whole content across the network, files on IPFS are stored in different nodes, and their address/identifier ([CID](https://docs.ipfs.io/concepts/content-addressing/)) is broadcast to a distributed hash table ([DHT](ipns://docs.ipfs.io/concepts/dht/)) that will be used to locate the content when someone tries to access it.
If there isn't at least one node in the network serving the content, it will not be available, so one good practice is to upload/pin the content to multiple nodes, which is possible due to the way [content is resolved in the IPFS network](https://docs.ipfs.io/concepts/how-ipfs-works/#distributed-hash-tables-dhts). The more nodes in the network are hosting your files, the faster it will be to find and retrieve them, and consequently, the lower will be the chances of downtime.

At this point, I already know how to spin up an IPFS cluster, but I'm lacking the physical infrastructure to run it. Usually, the raspberry pi is an easy go-to solution, but a 3 node cluster would require a bit more powerful hardware and even if it didn't, my first assumption was that I needed a highly available infrastructure, so I accepted the first tradeoff and succumbed wonders of the cloud.
Though I strongly believe that self-hosting your own stuff is in the path for decentralization at some point in the future, the entry barriers presented by self-hosting are still too high, and it would take a significant effort to build the infrastructure I thought I needed, eventually changing this project into something else. On his article about [self-hosting and decentralization](https://mccormick.cx/news/entries/on-self-hosting-and-decentralized-software) Chris McCormick states that those same entry barriers created a market for big companies like Amazon to remove them, ultimately leading to internet centralization. Even most of the websites on IPFS, are (probably) hosted by services like [Pinata](https://www.pinata.cloud/) or [Infura](https://infura.io/), which are awesome and reliable but exist because it's still complicated for an average user to self-host their own contents with the same level of availability and performance established by the industry leaders.

## Building the infrastructure

Setting up an IPFS node is quite simple, and the official docs have a [very comprehensive guide](http://docs.ipfs.io/install/server-infrastructure/#server-infrastructure). It's a good place to start but for a production environment with more than one server or component, at least an orchestration tool is recommended. If I were to start over today, I'd probably use docker for the whole setup, but I bumped into [ansible-ipfs-cluster](https://github.com/hsanjuan/ansible-ipfs-cluster) which works quite well and was a good opportunity to make some [contributions (pending merge)](https://github.com/hsanjuan/ansible-ipfs-cluster/pull/10) to the project itself. The `ipfs` daemon by connects to other peers in the network to serve its own content and exposes an HTTP API (Gateway) that can be used to retrieve any CID already uploaded to IPFS. Redundancy is achieved with [`ipfs-cluster`](https://cluster.ipfs.io), which synchronizes content across a swarm of IPFS nodes using [RAFT or CRDT consensus](https://cluster.ipfs.io/documentation/guides/consensus).

<Mermaid chart={`graph LR;
    subgraph madokeipfscluster [Madoke's IPFS Cluster]
        ipfs-cluster1 --> ipfs-cluster2 --> ipfs-cluster3 --> ipfs-cluster1
        subgraph node1 [node1]
            ipfs-cluster1 --> ipfs1
        end
        subgraph node2 [node2]
            ipfs-cluster2 --> ipfs2
        end
        subgraph node3 [node3]
            ipfs-cluster3 --> ipfs3
        end
    end
    ipfs1 --> other-ipfs-peers
    ipfs2 --> other-ipfs-peers
    ipfs3 --> other-ipfs-peers
`} />

### Exposing the website over HTTP

One of the greatest things about IPFS is that, for the sole purpose of sharing files like static webpages, the HTTP endpoint doesn't need to be exposed, and therefore we also don't need to worry about TLS certificates, Caching, DDoS, etc. As long as the files are stored by a node, any IPFS Gateway is able to retrieve them, making it possible to invert the logic of serving content online, by having the HTTP gateways pull content from the network using their nodes, provided that everyone would have access to its own private IPFS gateway and node. But we're not quite there yet, so in order to bridge he gap with the internet we use today, Cloudflare, Pinata, and many other companies adopting IPFS host public gateways so that we can access files [the same way we always did](https://gateway.ipfs.io/ipfs/QmZ4tDuvesekSs4qM5ZBKpXiZGun7S2CYtEZRB3DYXkjGx). To make it all even more seamless, a standard called [DNSLink](https://dnslink.dev) was proposed by [Protocol Labs](https://protocol.ai), the company behind IPFS, to allow linking a DNS record to any type of content.

Having said all of that, I chose to use Cloudflare's DNS Registrar and public IPFS Gateway (cloudflare-ipfs.com) because they have quite good [documentation](https://developers.cloudflare.com/distributed-web/ipfs-gateway) and support... and it's almost free. The image below illustrates the first setup and it pretty much resembles a common web stack: self-hosted files, hidden behind Cloudflare.

<Mermaid chart={`graph LR;
  browser
  self-hosted-ipfs-cluster
  subgraph Cloudflare
    cloudflare-ipfs-gateway
    cloudflare-ipfs-nodes
  end
  browser --> cloudflare-ipfs-gateway
  cloudflare-ipfs-gateway --> cloudflare-ipfs-nodes
  cloudflare-ipfs-nodes --> self-hosted-ipfs-cluster
`} />

### Deploying files to the cluster

At this point, adding files to the ipfs nodes was fairly simple when done manually, provided that the files were already available to the `ipfs` daemon. That's ok for experimenting but it becomes a little impractical for automating with a CI/CD tool like github actions, for example. It didn't took me too long to find [ipfs-deploy](https://github.com/ipfs-shipyard/ipfs-deploy), an npm package that is able to pin content to several different IPFS Pinning services, and change the DNSLink using Cloudflare's API. Getting it to work with the current setup was a different story. As said before, up until now, the ipfs cluster's HTTP API wasn't publicly available because it wasn't needed for accessing the files. However, `ipfs-deploy` and pretty much any other ipfs deployment tool will need the IPFS cluster API to upload and pin the files. I've started with a standard approach, nginx + letsencrypt, to at least have some access control and security (Auth and TLS termination), however an [issue with go-ipfs and nginx](https://github.com/ipfs/go-ipfs/blob/master/docs/production/reverse-proxy.md), which obviously only presented itself after everything was properly setup, forced me to directly expose the cluster API using its [security features](https://cluster.ipfs.io/documentation/reference/configuration/#restapi). The last problem down the road [was related to certbot](https://github.com/geerlingguy/ansible-role-certbot/issues/156), turns out that certbot runs as root, and if the user running `ipfs` and `ipfs-cluster` can't escalate privileges like standard nginx installations, it won't be able to use the certificates. It was solved with a good old `chmod 644`, which I'm not very proud of.

After all of this madness, the setup to host and deploy this website using IPFS was up and running, and madoke.org was available on IPFS. If you want to build something similar, I've shared the [ansible-playbook](https://github.com/madoke/ipfs-cluster-orchestration) that does all of this, hoping that it might be useful at some point in the future.

## Final Results

Building this infrastructure was an educative and challenging journey, which I think, many of us will need to go through in order to push the ecosystem forward. The results however, weren't that satisfying at the light of the expected returns.

### Censorship Resistance

Because I didn't want to go through the hassle of building the physical server infrastructure, the website can still be taken down by the cloud provider hosting my IPFS cluster, which could however be mitigated by uploading it to multiple nodes, apart from my own. Cloudflare can still take the website down by blocking it on their gateway but that can be prevented by self-hosting an IPFS gateway. As to the DNS, I couldn't find an alternative and this is probably one of the biggest problems the decentralized web has to solve. The [Ethereum Naming Service](https://ens.domains) (ENS) attempts to fix this problem with smart contracts, but even them are subject to ICANN.

### Performance

After every new deployment, the website would take up to 5 minutes to load, until it was cached. I tried a lot of several different combinations here, like multiple gateways (Cloudflare, IPFS, Pinata), pinning the files not only to my nodes, but also to Pinata and Infura, and last but not list, dozens of finetuning configuration in the IPFS cluster, like direct, explicit peering with Cloudflare and Pinata's nodes. None of this unfortunately solve the problem, and [a couple of searches](https://discuss.ipfs.io/t/debugging-slow-content-discovery/5440) revealed that there are many others experiencing similar issues. This was the main dealbreaker, and It was at this point that I've decided to take a couple of steps back to thing about what I was trying to build.

The [problem with public gateways](https://medium.com/pinata/the-ipfs-gateway-problem-64bbe7eb8170), as exposed by Matt Ober, CTO of Pinata, is that they were built because the vast majority of the web is still served over HTTP, and therefore, incompatible with distributed, peer to peer systems. They had to build IPFS gateways, so that normal users could experience IPFS in the internet that we know today. So at this point, I'm sending my files to my IPFS node, hosted on someone else's computer, and accessing them via someone else's HTTP gateway, which is exactly the same as the traditional web stack. Alternatively I could host my own IPFS gateway, so that content resolution could be faster, but then again, I'd be better off with nginx serving static files.

## Conclusions & Tradeoffs

There's no good solution to this, because I really want my website to be easily accessible and available to everyone, and at the moment of this writing, almost everyone still uses the traditional web browsers to navigate the web. In order to truly benefit from peer to peer and distributed file sharing protocols, the tools that we use to browse the intenet need to change their approach dramatically, moving away from centralized and outdated things like HTTP and DNS. [Brave](https://brave.com/), [Metamask](https://metamask.io/), and the [IPFS Companion](ipns://docs.ipfs.io/install/ipfs-companion/) are a good starting point, but we're sill lacking a strong standard and massive adoption to disrupt major browser developers (Google, Apple, Microsoft) and get them onboard too.

```bash
> dig +short TXT _dnslink.madoke.org
"dnslink=/ipfs/QmbKyCXSYiJPQdvB5Z2MEBNGfjTbTfAA9x4THsS1PSLcPX"
```
For now, this website went back to Cloudflare Pages, where it's served reliably and fast, but if you access it on a browser that supports DNSLink and `ipfs://` urls like Brave or IPFS Companion, you will be able to access its IPFS version, but downloading it from your own HTTP gateway/IPFS node that's connected to other IPFS nodes and will fetch the content like it was intended to, making full use of the peer to peer, distributed content network.



